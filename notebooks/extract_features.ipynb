{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# # stopwords: pronoun, wh-words. Maybe we should not remove these words\n",
    "\n",
    "\n",
    "# stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# list1 = raw_data.question1.values\n",
    "# for q in raw_data.question1.values:\n",
    "#     try:\n",
    "#         word_tokenize(q)\n",
    "#     except:\n",
    "#         print(q)\n",
    "#         break\n",
    "        \n",
    "# tokens_q1 = [word_tokenize(q) for q in raw_data.question1.values]\n",
    "# tokens_q2 = [word_tokenize(q) for q in raw_data.question2.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)  \n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_tokens(tokens, stemmer):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        return stemmed\n",
    "    \n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(translate_table)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "def process(corpus, max_features = 100):\n",
    "#     porterstem = PorterStemmer()\n",
    "#     porter_token = lambda text:tokenize(text, porterstem)\n",
    "#     tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, max_features = 100)\n",
    "    tfs = tfidf.fit(corpus)\n",
    "    return tfidf\n",
    "def tf_idf_vector(q, tfidf):\n",
    "    return pd.DataFrame(tfidf.transform([q]).toarray())\n",
    "\n",
    "def tf_idf_similarity(q1, q2, tfidf):\n",
    "    m = tfidf.transform([q1, q2])\n",
    "    return cosine_similarity(m[0], m[1])\n",
    "\n",
    "def feature_eng_tfidf(df, tfidf = None):\n",
    "    max_features = 100\n",
    "    corpus = list(qpair['question1']) + list(qpair['question2'])\n",
    "    if tfidf is None:\n",
    "        tfidf = process(corpus, max_features = max_features)\n",
    "#     getvec = lambda q: tf_idf_vector(q, tfidf)\n",
    "    \n",
    "#     q1tfidf = df['question1'].apply(getvec)\n",
    "    q1tfidf = pd.DataFrame()\n",
    "    \n",
    "#     q2tfidf = df['question2'].apply(getvec)\n",
    "    q2tfidf = pd.DataFrame()\n",
    "    for idx, r in df.iterrows():\n",
    "        v1 = tf_idf_vector(r['question1'], tfidf)\n",
    "        v2 = tf_idf_vector(r['question2'], tfidf)\n",
    "        q1tfidf = q1tfidf.append(v1, ignore_index=True)\n",
    "        q2tfidf = q2tfidf.append(v2, ignore_index=True)\n",
    "    q1tfidf.columns = [\"q1tfidx_\"+str(i) for i in range(max_features)]\n",
    "    q2tfidf.columns = [\"q2tfidx_\"+str(i) for i in range(max_features)]\n",
    "    \n",
    "    q1tfidf.index = df.index\n",
    "    q2tfidf.index = df.index\n",
    "    df.reset_index()\n",
    "    \n",
    "    df = pd.concat([df, q1tfidf, q2tfidf], axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wenyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "qpair = pd.read_csv(\"../data/qpairs_dev.csv\")\n",
    "qpair = qpair.dropna()\n",
    "corpus = list(qpair['question1']) + list(qpair['question2'])\n",
    "tfidf = process(corpus, 1000)\n",
    "\n",
    "\n",
    "# for i in range(50):\n",
    "#     q1 = qpair.loc[i]['question1']\n",
    "#     q2 = qpair.loc[0]['question2']\n",
    "#     print(tf_idf_vector(q1,tfidf))\n",
    "#     print(tf_idf_similarity(q1, q2, tfidf))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
      "\n",
      "                                           question2  is_duplicate  q1tfidx_0  \\\n",
      "0  What is the step by step guide to invest in sh...             0        0.0   \n",
      "1  What would happen if the Indian government sto...             0        0.0   \n",
      "2  Find the remainder when [math]23^{24}[/math] i...             0        0.0   \n",
      "\n",
      "   q1tfidx_1  q1tfidx_2  q1tfidx_3     ...      q2tfidx_90  q2tfidx_91  \\\n",
      "0        0.0        0.0        0.0     ...             0.0         0.0   \n",
      "1        0.0        0.0        0.0     ...             0.0         0.0   \n",
      "2        0.0        0.0        0.0     ...             0.0         0.0   \n",
      "\n",
      "   q2tfidx_92  q2tfidx_93  q2tfidx_94  q2tfidx_95  q2tfidx_96  q2tfidx_97  \\\n",
      "0         0.0         0.0         0.0         0.0    0.000000         0.0   \n",
      "1         0.0         0.0         0.0         0.0    0.501717         0.0   \n",
      "2         0.0         0.0         0.0         0.0    0.000000         0.0   \n",
      "\n",
      "   q2tfidx_98  q2tfidx_99  \n",
      "0         0.0         0.0  \n",
      "1         0.0         0.0  \n",
      "2         0.0         0.0  \n",
      "\n",
      "[3 rows x 206 columns]\n"
     ]
    }
   ],
   "source": [
    "qpair = feature_eng(qpair)\n",
    "tfidfqpair = feature_eng_tfidf(qpair,tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')\n",
      "(1, 100)\n",
      "(3, 100) (3, 6)\n",
      "Int64Index([0, 1, 2], dtype='int64')\n",
      "   q1tfidx_0  q1tfidx_1  q1tfidx_2  q1tfidx_3  q1tfidx_4  q1tfidx_5  \\\n",
      "0        0.0        0.0        0.0        0.0   0.000000        0.0   \n",
      "1        0.0        0.0        0.0        0.0   0.000000        0.0   \n",
      "2        0.0        0.0        0.0        0.0   0.552623        0.0   \n",
      "\n",
      "   q1tfidx_6  q1tfidx_7  q1tfidx_8  q1tfidx_9      ...       q1tfidx_96  \\\n",
      "0        0.0        0.0        0.0        0.0      ...              0.0   \n",
      "1        0.0        0.0        0.0        0.0      ...              0.0   \n",
      "2        0.0        0.0        0.0        0.0      ...              0.0   \n",
      "\n",
      "   q1tfidx_97  q1tfidx_98  q1tfidx_99  id  qid1  qid2  \\\n",
      "0         0.0         0.0         0.0   0     1     2   \n",
      "1         0.0         0.0         0.0   1     3     4   \n",
      "2         0.0         0.0         0.0   3     7     8   \n",
      "\n",
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2  Why am I mentally very lonely? How can I solve...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "\n",
      "[3 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "qshort = qpair[0:3]\n",
    "print(qshort.columns)\n",
    "\n",
    "print(tf_idf_vector(corpus[0], tfidf).shape)\n",
    "q1tfidf = pd.DataFrame()\n",
    "for idx, r in qshort.iterrows():\n",
    "    v1 = tf_idf_vector(r['question1'], tfidf)\n",
    "#     print(v1)\n",
    "    q1tfidf=q1tfidf.append(v1)\n",
    "#     print(q1tfidf)\n",
    "q1tfidf.columns = [\"q1tfidx_\"+str(i) for i in range(100)]\n",
    "q1tfidf.index=qshort.index\n",
    "# qshort.reset_index(inplace=True)\n",
    "print(q1tfidf.shape, qshort.shape)\n",
    "print(q1tfidf.index)\n",
    "cc = pd.concat([q1tfidf, qshort],axis=1)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# q1tfidf = pd.DataFrame(tf_idf_vector(corpus[0], tfidf).toarray())\n",
    "# q1tfidf.columns = [\"q1tfidf_\"+str(i) for i in range(100)]\n",
    "getvec = lambda q: tf_idf_vector(q, tfidf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos_tag \n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# def pos_synnet_similarity(q1, q2):\n",
    "#     q1 = nltk.word_tokenize(q1)\n",
    "#     q2 = tokenize(q2)\n",
    "#     tag1 = nltk.pos_tag(q1)\n",
    "#     tag2 = nltk.pos_tag(q2)\n",
    "    \n",
    "#     print(tag1, tag2)\n",
    "\n",
    "\n",
    "    \n",
    "# print(corpus[1], corpus[2])\n",
    "# pos_synnet_similarity(corpus[1], corpus[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import re\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "UNI_BLEU_WEIGHTS = (1, 0, 0, 0)\n",
    "BI_BLEU_WEIGHTS = (0, 1, 0, 0)\n",
    "BLEU2_WEIGHTS = (0.5, 0.5, 0, 0)\n",
    "\n",
    "\n",
    "# def save_results(predictions, IDs, filename):\n",
    "#     with open(filename, 'w') as f:\n",
    "#         f.write(\"test_id,is_duplicate\\n\")\n",
    "#         for i, pred in enumerate(predictions):\n",
    "#             f.write(\"%d,%f\\n\" % (IDs[i], pred))\n",
    "\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence by space into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "\n",
    "\n",
    "def char_ngram_tokenizer(sentence, n):\n",
    "    \"\"\"Character ngram tokenizer: split the sentence into a list of char ngram tokens.\"\"\"\n",
    "    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]\n",
    "\n",
    "\n",
    "def get_word_count(x):\n",
    "    return len(tokenizer(str(x)))\n",
    "\n",
    "def word_overlap(x):\n",
    "    return len(set(str(x['question1']).lower().split()).intersection(\n",
    "             set(str(x['question2']).lower().split())))\n",
    "\n",
    "\n",
    "def char_bigram_overlap(x):\n",
    "    return len(set(char_ngram_tokenizer(str(x['question1']), 2)).intersection(\n",
    "             set(char_ngram_tokenizer(str(x['question2']), 2))))\n",
    "\n",
    "\n",
    "def char_trigram_overlap(x):\n",
    "    return len(set(char_ngram_tokenizer(str(x['question1']), 3)).intersection(\n",
    "             set(char_ngram_tokenizer(str(x['question2']), 3))))\n",
    "\n",
    "\n",
    "def char_fourgram_overlap(x):\n",
    "    return len(set(char_ngram_tokenizer(str(x['question1']), 4)).intersection(\n",
    "             set(char_ngram_tokenizer(str(x['question2']), 4))))\n",
    "\n",
    "\n",
    "def get_uni_BLEU(x):\n",
    "    s_function = SmoothingFunction()\n",
    "    # method 2 is add 1 smoothing\n",
    "    return sentence_bleu([tokenizer(str(x['question2']))],\n",
    "                         tokenizer(str(x['question1'])),\n",
    "                         weights=UNI_BLEU_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2)\n",
    "\n",
    "\n",
    "def get_bi_BLEU(x):\n",
    "    s_function = SmoothingFunction()\n",
    "    # method 2 is add 1 smoothing\n",
    "    return sentence_bleu([tokenizer(str(x['question2']))],\n",
    "                         tokenizer(str(x['question1'])),\n",
    "                         weights=BI_BLEU_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2)\n",
    "\n",
    "\n",
    "def get_BLEU2(x):\n",
    "    s_function = SmoothingFunction()\n",
    "    # method 2 is add 1 smoothing\n",
    "    return sentence_bleu([tokenizer(str(x['question2']))],\n",
    "                         tokenizer(str(x['question1'])),\n",
    "                         weights=BLEU2_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2)\n",
    "\n",
    "\n",
    "def feature_eng(df):\n",
    "\n",
    "    # word count of question 1\n",
    "    df['q1_word_count'] = df['question1'].apply(get_word_count)\n",
    "\n",
    "    # word count of question 2\n",
    "    df['q2_word_count'] = df['question2'].apply(get_word_count)\n",
    "\n",
    "    # word count difference\n",
    "    df['word_count_diff'] = abs(df['q1_word_count'] - df['q2_word_count'])\n",
    "\n",
    "    # number of word overlap between q1 and q2\n",
    "    df['word_overlap'] = df.apply(word_overlap, axis=1)\n",
    "\n",
    "    # unigram BLEU score\n",
    "    df['uni_BLEU'] = df.apply(get_uni_BLEU, axis=1)\n",
    "\n",
    "    # bigram BLEU score\n",
    "    df['bi_BLEU'] = df.apply(get_bi_BLEU, axis=1)\n",
    "\n",
    "    # BLEU2 score\n",
    "    df['BLEU2'] = df.apply(get_BLEU2, axis=1)\n",
    "\n",
    "    # character unigram overlap\n",
    "    df['char_bigram_overlap'] = df.apply(char_bigram_overlap, axis=1)\n",
    "\n",
    "    # character trigram overlap\n",
    "    df['char_trigram_overlap'] = df.apply(char_trigram_overlap, axis=1)\n",
    "\n",
    "    # character 4-gram overlap\n",
    "    df['char_4gram_overlap'] = df.apply(char_fourgram_overlap, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/wenyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Synset similarity \n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Parameters to the algorithm. Currently set to values that was reported\n",
    "# in the paper to produce \"best\" results.\n",
    "ALPHA = 0.2\n",
    "BETA = 0.45\n",
    "ETA = 0.4\n",
    "PHI = 0.2\n",
    "DELTA = 0.85\n",
    "\n",
    "brown_freqs = dict()\n",
    "N = 0\n",
    "\n",
    "######################### word similarity ##########################\n",
    "\n",
    "def get_best_synset_pair(word_1, word_2):\n",
    "    \"\"\" \n",
    "    Choose the pair with highest path similarity among all pairs. \n",
    "    Mimics pattern-seeking behavior of humans.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    synsets_2 = wn.synsets(word_2)\n",
    "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        max_sim = -1.0\n",
    "        best_pair = None, None\n",
    "        for synset_1 in synsets_1:\n",
    "            for synset_2 in synsets_2:\n",
    "                sim = wn.path_similarity(synset_1, synset_2)\n",
    "#                 print(sim)\n",
    "                if sim != None and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    best_pair = synset_1, synset_2\n",
    "        return best_pair\n",
    "\n",
    "def length_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of the length of the shortest path in the semantic \n",
    "    ontology (Wordnet in our case as well as the paper's) between two \n",
    "    synsets.\n",
    "    \"\"\"\n",
    "    l_dist = math.inf\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return 0.0\n",
    "    if synset_1 == synset_2:\n",
    "        # if synset_1 and synset_2 are the same synset return 0\n",
    "        l_dist = 0.0\n",
    "    else:\n",
    "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])        \n",
    "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
    "        if len(wset_1.intersection(wset_2)) > 0:\n",
    "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
    "            l_dist = 1.0\n",
    "        else:\n",
    "            # just compute the shortest path between the two\n",
    "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
    "            if l_dist is None:\n",
    "                l_dist = 0.0\n",
    "    # normalize path length to the range [0,1]\n",
    "    return math.exp(-ALPHA * l_dist)\n",
    "\n",
    "def hierarchy_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of depth in the ontology to model the fact that \n",
    "    nodes closer to the root are broader and have less semantic similarity\n",
    "    than nodes further away from the root.\n",
    "    \"\"\"\n",
    "    h_dist = math.inf\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return h_dist\n",
    "    if synset_1 == synset_2:\n",
    "        # return the depth of one of synset_1 or synset_2\n",
    "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
    "    else:\n",
    "        # find the max depth of least common subsumer\n",
    "        hypernyms_1 = {x[0]:x[1] for x in synset_1.hypernym_distances()}\n",
    "        hypernyms_2 = {x[0]:x[1] for x in synset_2.hypernym_distances()}\n",
    "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
    "            set(hypernyms_2.keys()))\n",
    "        if len(lcs_candidates) > 0:\n",
    "            lcs_dists = []\n",
    "            for lcs_candidate in lcs_candidates:\n",
    "                lcs_d1 = 0\n",
    "                if lcs_candidate not in hypernyms_1:\n",
    "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
    "                lcs_d2 = 0\n",
    "                if lcs_candidate not in hypernyms_2:\n",
    "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
    "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
    "            h_dist = max(lcs_dists)\n",
    "        else:\n",
    "            h_dist = 0\n",
    "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) / \n",
    "        (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
    "    \n",
    "def word_similarity(word_1, word_2):\n",
    "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
    "    return (length_dist(synset_pair[0], synset_pair[1]) * \n",
    "        hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
    "\n",
    "######################### sentence similarity ##########################\n",
    "\n",
    "def most_similar_word(word, word_set):\n",
    "    \"\"\"\n",
    "    Find the word in the joint word set that is most similar to the word\n",
    "    passed in. We use the algorithm above to compute word similarity between\n",
    "    the word and each word in the joint word set, and return the most similar\n",
    "    word and the actual similarity value.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    sim_word = \"\"\n",
    "    for ref_word in word_set:\n",
    "        sim = word_similarity(word, ref_word)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            sim_word = ref_word\n",
    "    return sim_word, max_sim\n",
    "    \n",
    "def info_content(lookup_word):\n",
    "    \"\"\"\n",
    "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
    "    smoothed frequency distribution of words, then uses this information\n",
    "    to compute the information content of the lookup_word.\n",
    "    \"\"\"\n",
    "    global N\n",
    "    if N == 0:\n",
    "        # poor man's lazy evaluation\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if word not in brown_freqs:\n",
    "                    brown_freqs[word] = 0\n",
    "                brown_freqs[word] = brown_freqs[word] + 1\n",
    "                N = N + 1\n",
    "    lookup_word = lookup_word.lower()\n",
    "    n = 0 if lookup_word not in brown_freqs else brown_freqs[lookup_word]\n",
    "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
    "    \n",
    "def semantic_vector(words, joint_words, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
    "    a collection of words. The size of the semantic vector is the same as the\n",
    "    size of the joint word set. The elements are 1 if a word in the sentence\n",
    "    already exists in the joint word set, or the similarity of the word to the\n",
    "    most similar word in the joint word set if it doesn't. Both values are \n",
    "    further normalized by the word's (and similar word's) information content\n",
    "    if info_content_norm is True.\n",
    "    \"\"\"\n",
    "    sent_set = set(words)\n",
    "    semvec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in sent_set:\n",
    "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
    "            semvec[i] = 1.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
    "        else:\n",
    "            # find the most similar word in the joint set and set the sim value\n",
    "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
    "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
    "        i = i + 1\n",
    "    return semvec                \n",
    "            \n",
    "def semantic_similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic similarity between two sentences as the cosine\n",
    "    similarity between the semantic vectors computed for each sentence.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = set(words_1).union(set(words_2))\n",
    "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
    "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
    "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
    "\n",
    "######################### word order similarity ##########################\n",
    "\n",
    "def word_order_vector(words, joint_words, windex):\n",
    "    \"\"\"\n",
    "    Computes the word order vector for a sentence. The sentence is passed\n",
    "    in as a collection of words. The size of the word order vector is the\n",
    "    same as the size of the joint word set. The elements of the word order\n",
    "    vector are the position mapping (from the windex dictionary) of the \n",
    "    word in the joint set if the word exists in the sentence. If the word\n",
    "    does not exist in the sentence, then the value of the element is the \n",
    "    position of the most similar word in the sentence as long as the similarity\n",
    "    is above the threshold ETA.\n",
    "    \"\"\"\n",
    "    wovec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    wordset = set(words)\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in wordset:\n",
    "            # word in joint_words found in sentence, just populate the index\n",
    "            wovec[i] = windex[joint_word]\n",
    "        else:\n",
    "            # word not in joint_words, find most similar word and populate\n",
    "            # word_vector with the thresholded similarity\n",
    "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
    "            if max_sim > ETA:\n",
    "                wovec[i] = windex[sim_word]\n",
    "            else:\n",
    "                wovec[i] = 0\n",
    "        i = i + 1\n",
    "    return wovec\n",
    "\n",
    "def word_order_similarity(sentence_1, sentence_2):\n",
    "    \"\"\"\n",
    "    Computes the word-order similarity between two sentences as the normalized\n",
    "    difference of word order between the two sentences.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = list(set(words_1).union(set(words_2)))\n",
    "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
    "    r1 = word_order_vector(words_1, joint_words, windex)\n",
    "    r2 = word_order_vector(words_2, joint_words, windex)\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "\n",
    "######################### overall similarity ##########################\n",
    "\n",
    "def similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Calculate the semantic similarity between two sentences. The last \n",
    "    parameter is True or False depending on whether information content\n",
    "    normalization is desired or not.\n",
    "    \"\"\"\n",
    "    return DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) + \\\n",
    "        (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)\n",
    "def feature_eng_semantic(df):\n",
    "    sim = []\n",
    "    for idx, r in df.iterrows():\n",
    "        sim.append(similarity(r['question1'], r['question2'], True))\n",
    "    df['sem_sim'] = sim\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  qid1  qid2                                          question1  \\\n",
      "0   0     1     2  What is the step by step guide to invest in sh...   \n",
      "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "\n",
      "                                           question2  is_duplicate   sem_sim  \n",
      "0  What is the step by step guide to invest in sh...             0  0.938124  \n",
      "1  What would happen if the Indian government sto...             0  0.670906  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/ipykernel_launcher.py:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin: 1544329427.808866\n",
      "qp2: 1544329674.245862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-340-1b0dde66ba38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qp2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# qpair2 = feature_eng_semantic(qpair1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mqpair2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_eng_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqpair1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-4216fb8e79b8>\u001b[0m in \u001b[0;36mfeature_eng_tfidf\u001b[0;34m(df, tfidf)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mq1tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mq2tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq2tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mq1tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"q1tfidx_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6209\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6210\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6211\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6213\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5416\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5417\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 5418\u001b[0;31m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/defaultenv/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m    367\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[0;32m--> 368\u001b[0;31m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[1;32m    369\u001b[0m         return self.make_block_same_class(\n\u001b[1;32m    370\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"begin:\", time.time())\n",
    "qpair1 = feature_eng(qpair)\n",
    "print(\"qp2:\", time.time())\n",
    "# qpair2 = feature_eng_semantic(qpair1)\n",
    "qpair2 = feature_eng_tfidf(qpair1)\n",
    "print(\"done:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
